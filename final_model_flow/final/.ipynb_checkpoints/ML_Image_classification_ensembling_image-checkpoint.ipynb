{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ffc189-81d0-4023-85fb-051ec9282c23",
   "metadata": {},
   "source": [
    "Importing all the necessary Libraries required for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "029068d7-6b91-4125-8471-508f0a53ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d34dd-895f-44a3-b384-3892aac52fb8",
   "metadata": {},
   "source": [
    "# **1. Feature Selection - Forward Feature selection method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0480a7-51cd-4cc9-a6a0-2ac0780bbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature(csv):\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(csv)\n",
    "        # Split the data into features and target\n",
    "        X = df.drop(columns=[df.columns[0],df.columns[-1]])  # assuming first and last columns are not features\n",
    "        y = df[df.columns[-1]]\n",
    "        \n",
    "        # Splitting dataset into training and testing\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Standardizing the features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Initialize classifiers for each selection method\n",
    "        knn_forward = KNeighborsClassifier(n_neighbors=3)\n",
    "        knn_backward = KNeighborsClassifier(n_neighbors=3)\n",
    "        svc_rfe = SVC(kernel=\"linear\")\n",
    "        \n",
    "        # Forward Feature Selection\n",
    "        sfs_forward = SFS(knn_forward,\n",
    "                          k_features='best',\n",
    "                          forward=True,\n",
    "                          floating=False,\n",
    "                          scoring='accuracy',\n",
    "                          cv=5)\n",
    "        sfs_forward = sfs_forward.fit(X_train_scaled, y_train)\n",
    "        X_train_sfs_forward = sfs_forward.transform(X_train_scaled)\n",
    "        X_test_sfs_forward = sfs_forward.transform(X_test_scaled)\n",
    "        knn_forward.fit(X_train_sfs_forward, y_train)\n",
    "        \n",
    "        \n",
    "        # Prediction and Accuracy Calculation\n",
    "        y_pred_forward = knn_forward.predict(X_test_sfs_forward)\n",
    "        acc_forward = accuracy_score(y_test, y_pred_forward)\n",
    "        \n",
    "        # Comparison Matrix\n",
    "        comparison_matrix = pd.DataFrame({\n",
    "            'Method': ['Forward Selection'],\n",
    "            'Selected Features': [sfs_forward.k_feature_idx_],\n",
    "            'Accuracy': [acc_forward]\n",
    "        })\n",
    "        \n",
    "        # Set display options to show all rows and columns\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        \n",
    "        # Print the DataFrame\n",
    "        print(comparison_matrix)\n",
    "        \n",
    "        # Print the selected feature names for each method\n",
    "        print(\"Forward Selection Feature Names:\", df.columns[list(sfs_forward.k_feature_idx_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f53be-5c3e-4aba-a93a-9ad17188a48e",
   "metadata": {},
   "source": [
    "Forward and backward feature selections both have pretty good accuray. Though the accuracy is better for backward lets use the features selected by the forward selection because of the computational efficiency and scalability in this case where the feature set and number of images are very large. Forward feature selection allows the addition of features in a stepwise manner, isolating the important features in the first few steps of the procedure without computational overloading. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a8cce-b146-4f50-8f85-643cdcb9944c",
   "metadata": {},
   "source": [
    "# **2. Classification Of Chunks and Ensembling all the model predictions for the chunks after feature selection.**\n",
    "# Classification: Train and Ensemble Models\n",
    "Here, multiple machine learning classifiers are trained, and an ensemble method is used to combine their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b815e63f-b1b2-4955-9611-7654d1d95040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classi(train,dest):\n",
    "            # Load the training dataset\n",
    "            training_data = pd.read_csv(train)\n",
    "            \n",
    "            selected_features = [\n",
    "                'LocalEnergy_0_1', 'LocalEnergy_0_2',\n",
    "                'LocalEnergy_0.7853981633974483_1', 'LocalEnergy_0.7853981633974483_2',\n",
    "                'LocalEnergy_0.7853981633974483_3', 'LocalEnergy_1.5707963267948966_1',\n",
    "                'LocalEnergy_1.5707963267948966_2', 'LocalEnergy_1.5707963267948966_3',\n",
    "                'LocalEnergy_2.356194490192345_1', 'LocalEnergy_2.356194490192345_2',\n",
    "                'LocalEnergy_2.356194490192345_3', 'MeanAmplitude_0_1',\n",
    "                'MeanAmplitude_0_2', 'MeanAmplitude_0_3',\n",
    "                'MeanAmplitude_0.7853981633974483_1',\n",
    "                'MeanAmplitude_0.7853981633974483_2',\n",
    "                'MeanAmplitude_0.7853981633974483_3',\n",
    "                'MeanAmplitude_1.5707963267948966_1',\n",
    "                'MeanAmplitude_1.5707963267948966_2',\n",
    "                'MeanAmplitude_1.5707963267948966_3',\n",
    "                'MeanAmplitude_2.356194490192345_1',\n",
    "                'MeanAmplitude_2.356194490192345_2',\n",
    "            ]\n",
    "            \n",
    "            # Separate features (X) and labels (y)\n",
    "            y = training_data[\"class\"]\n",
    "            X = training_data[selected_features]\n",
    "            \n",
    "            # Normalize features\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            # Encode the class labels using label encoding\n",
    "            encoder = LabelEncoder()\n",
    "            y_encoded = encoder.fit_transform(y)\n",
    "            \n",
    "            # Split the data into training and testing sets, retaining indices\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # Determine the optimal k for K-Nearest Neighbors\n",
    "            k_range = range(1, 21)\n",
    "            knn_scores = []\n",
    "            for k in k_range:\n",
    "                knn = KNeighborsClassifier(n_neighbors=k)\n",
    "                scores = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "                knn_scores.append(scores.mean())\n",
    "            plt.plot(k_range, knn_scores, marker='o')\n",
    "            plt.xlabel('Number of Neighbors k')\n",
    "            plt.ylabel('Cross-Validated Accuracy')\n",
    "            plt.title('Elbow Method for Optimal k')\n",
    "            plt.show()\n",
    "            best_k = k_range[knn_scores.index(max(knn_scores))]\n",
    "            print(f\"Optimal k for K-Nearest Neighbors: {best_k}\")\n",
    "            \n",
    "            # Define classifiers with optimal parameters\n",
    "            classifiers = {    \n",
    "                \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "                \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=best_k),\n",
    "                \"MLP Classifier\": MLPClassifier(random_state=42, max_iter=1000),\n",
    "                \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "                \"Bagging (Random Forest)\": BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "            }\n",
    "            # Initialize DataFrame to store predictions\n",
    "            predictions_df = pd.DataFrame(columns=[\"ImageName\"])\n",
    "            \n",
    "            # Store predictions made by each classifier\n",
    "            for name, classifier in classifiers.items():\n",
    "                # Train classifier\n",
    "                classifier.fit(X_train, y_train)\n",
    "                \n",
    "                # Load the new dataset\n",
    "                new_data = pd.read_csv(dest+r\"\\palm_document_Gabor_gamma_0.1_sigma_0.5_phi_0.csv\")\n",
    "                X_new = new_data[selected_features]  # Use the same columns as training data\n",
    "                X_new_scaled = scaler.transform(X_new)  # Use the same scaler as used for training data\n",
    "            \n",
    "                # Make predictions\n",
    "                predictions = classifier.predict(X_new_scaled)\n",
    "                \n",
    "                \n",
    "                # Inverse transform the encoded predictions to get class names\n",
    "                predictions = encoder.inverse_transform(predictions)\n",
    "                \n",
    "                # Add predictions to the DataFrame\n",
    "                predictions_df[name] = predictions\n",
    "            \n",
    "            # Ensemble methods - Voting Classifier and Weighted Average\n",
    "            voting_ensemble = VotingClassifier(estimators=list(classifiers.items()))\n",
    "            voting_ensemble.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions using Voting Classifier\n",
    "            voting_predictions = voting_ensemble.predict(X_new_scaled)\n",
    "            \n",
    "            # Inverse transform the encoded predictions to get class names\n",
    "            voting_predictions = encoder.inverse_transform(voting_predictions)\n",
    "            \n",
    "            predictions_df[\"Voting_Ensemble\"] = voting_predictions\n",
    "            \n",
    "            # Add original class labels to the DataFrame\n",
    "            predictions_df[\"ImageName\"] = new_data[\"ImageName\"]\n",
    "            \n",
    "            # Save predictions to Excel\n",
    "            output_file = dest+r\"\\predictions_with_ensemble_after_feature_selection.xlsx\"\n",
    "            predictions_df.to_excel(output_file, index=False)\n",
    "            \n",
    "            print(\"Predictions saved to:\", output_file)\n",
    "            \n",
    "            # Create a Word document\n",
    "            doc = Document()\n",
    "            \n",
    "            # Add title to the document\n",
    "            doc.add_heading('Model Performance Metrics', level=1)\n",
    "            \n",
    "            for name, classifier in classifiers.items():\n",
    "                # Make predictions on the training data\n",
    "                train_predictions = classifier.predict(X_train)\n",
    "                \n",
    "                # Calculate classification report\n",
    "                report = classification_report(y_train, train_predictions, output_dict=True)\n",
    "                \n",
    "                # Add model name to the document\n",
    "                doc.add_heading(name, level=2)\n",
    "                \n",
    "                # Add precision table to the document\n",
    "                precision_data = report['weighted avg']\n",
    "                precision_table = doc.add_table(rows=2, cols=len(precision_data), style='Table Grid')\n",
    "                hdr_cells = precision_table.rows[0].cells\n",
    "                for i, (key, value) in enumerate(precision_data.items()):\n",
    "                    hdr_cells[i].text = key\n",
    "                row_cells = precision_table.rows[1].cells\n",
    "                for i, value in enumerate(precision_data.values()):\n",
    "                    row_cells[i].text = str(round(value, 2))\n",
    "                \n",
    "                # Add classification report to the document\n",
    "                doc.add_paragraph(\"Classification Report:\")\n",
    "                report_text = classification_report(y_train, train_predictions, target_names=encoder.classes_)\n",
    "                doc.add_paragraph(report_text)\n",
    "            \n",
    "            voting_train_predictions = voting_ensemble.predict(X_train)\n",
    "            voting_report = classification_report(y_train, voting_train_predictions, output_dict=True)\n",
    "            \n",
    "            doc.add_heading(\"Voting Ensemble\", level=2)\n",
    "            \n",
    "            precision_data = voting_report['weighted avg']\n",
    "            precision_table = doc.add_table(rows=2, cols=len(precision_data), style='Table Grid')\n",
    "            hdr_cells = precision_table.rows[0].cells\n",
    "            for i, (key, value) in enumerate(precision_data.items()):\n",
    "                hdr_cells[i].text = key\n",
    "            row_cells = precision_table.rows[1].cells\n",
    "            for i, value in enumerate(precision_data.values()):\n",
    "                row_cells[i].text = str(round(value, 2))\n",
    "            \n",
    "            doc.add_paragraph(\"Classification Report:\")\n",
    "            report_text = classification_report(y_train, voting_train_predictions, target_names=encoder.classes_)\n",
    "            doc.add_paragraph(report_text)\n",
    "            \n",
    "            # Save the document\n",
    "            doc.save(dest+r\"\\Model_performance_with_feature_selection.docx\")\n",
    "            \n",
    "            print(\"Model performance metrics saved to Word document.\")\n",
    "\n",
    "              # Function to generate LIME explanations\n",
    "            def generate_lime_explanation(model, data_point, feature_names):\n",
    "                explainer = lime.lime_tabular.LimeTabularExplainer(X_new_scaled, feature_names=feature_names, class_names=encoder.classes_, discretize_continuous=True)\n",
    "                explanation = explainer.explain_instance(data_point, model.predict_proba, num_features=len(feature_names))\n",
    "                return explanation\n",
    "            \n",
    "            # Generate LIME explanations for a few data points after making predictions\n",
    "            for name, classifier in classifiers.items():\n",
    "                print(f\"{name}\\n\\n\")\n",
    "                \n",
    "                for index, data_point in enumerate(X_new_scaled):\n",
    "                    if index == 15:  # Limiting to 15 data points for explanation\n",
    "                        break\n",
    "                    print(f\"Data Point {index} = {data_point}\")\n",
    "                    explanation = generate_lime_explanation(classifier, data_point, selected_features)\n",
    "                    explanation.show_in_notebook()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d1625-56ac-46d7-ba46-03f83b8a4e91",
   "metadata": {},
   "source": [
    "# **4. Integrating all the chunks to Images**\n",
    "\n",
    "In this section all the chunk classificstion are integrated into one single image and the final classifications of images are done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26155fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def integrate(dest):\n",
    "            # Load the dataset\n",
    "            df = pd.read_excel(dest+r\"\\predictions_with_ensemble_after_feature_selection.xlsx\")\n",
    "            \n",
    "            # Extract image names without chunk numbers\n",
    "            a = df['ImageName'].values\n",
    "            \n",
    "            # Group images based on common parent names\n",
    "            b = defaultdict(list)\n",
    "            for i in range(len(a)):\n",
    "                s = a[i]\n",
    "                s1 = s.rfind('_')\n",
    "                s1 = s[:s1]\n",
    "                b[s1].append(a[i])\n",
    "            \n",
    "            # Define ensembling methods\n",
    "            def voting_ensemble(predictions):\n",
    "                # Count the occurrences of each class label\n",
    "                vote_count = defaultdict(int)\n",
    "                for prediction in predictions:\n",
    "                    vote_count[prediction] += 1\n",
    "                \n",
    "                # Find the class label with the highest count\n",
    "                final_prediction = max(vote_count, key=vote_count.get)\n",
    "                return final_prediction\n",
    "            \n",
    "            # Create a DataFrame to store the output\n",
    "            output_data = []\n",
    "            \n",
    "            # Iterate through each key in the dictionary and classify the images using voting ensemble\n",
    "            for key, items in b.items():\n",
    "                # Combine all items associated with the key into one string\n",
    "                items_str = ', '.join(items)\n",
    "                row = {'ImageName': key, 'Items': items_str}  # Include the 'Items' column\n",
    "                item_df = df[df['ImageName'].isin(items)]\n",
    "                for column in item_df.columns[1:]:\n",
    "                    predictions = item_df[column].values\n",
    "                    prediction = voting_ensemble(predictions)\n",
    "                    row[column] = prediction\n",
    "                output_data.append(row)\n",
    "            \n",
    "            output_df = pd.DataFrame(output_data)\n",
    "            \n",
    "            # Reorder columns to have 'Items' as the second column\n",
    "            output_df = output_df[['ImageName', 'Items'] + [col for col in output_df.columns if col not in ['ImageName', 'Items']]]\n",
    "            \n",
    "            # Remove duplicate rows (if any)\n",
    "            output_df = output_df.drop_duplicates()\n",
    "            \n",
    "            # Save the output DataFrame to an Excel file\n",
    "            output_df.to_excel(dest+r\"\\output_fe.xlsx\", index=False)\n",
    "            print(\"Output printed\")\n",
    "            \n",
    "            # Print the values present in the last available column\n",
    "            last_column = output_df.columns[-1]\n",
    "            return(output_df[last_column].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fefebe0-9f93-4e9e-918a-b668cf61e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(dest,train):\n",
    "    feature(train)\n",
    "    classi(train,dest)\n",
    "    label=integrate(dest)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2fe86-266b-4734-8c74-159d9fb8d4be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
